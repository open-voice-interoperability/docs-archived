# Technical Masterplan

## Abstract

The Open Voice Network (OVON) is a non-profit industry association dedicated to the communal development and broad adoption of technical standards and ethical use guidelines for the emerging world of voice assistance.  It is a directed fund of the Linux Foundation, and independently funded and governed. 

The OVON Master Plan is a directional document thas defines the initiative's scope of the work and describes the open standards needed to realize a trusted, open implementation for voice assistants and related services.

The sections below map to OVON working groups and anticipated proposals for standards, TCK's, reference implementations, and usage guidelines. 

## Why the Open Voice Network?

The Open Voice Network was founded on these beliefs:

- **We are in the early days of voice assistance.**  We believe the future of voice assistance is **"multi"** -- a world of innumerable enterprise conversational agents operating across multiple platforms, accessed through multiple types of devices, accompanied by multiple modalities, and contextually integrated.  Today, the world of voice assistance is dominated by proprietary, cloud-centric platforms (and related devices) that deliver services using closed implementations.  While closed systems can create value for stakeholders and selected partners, a lack of standards-based interoperability and governance restricts the growth (and economic value) of a larger voice ecosystem. 
- **A broad lack of user trust inhibits investment and innovation in voice.**  A significant barrier to the realization of user value in voice assistance is the lack of trust that users (from consumers to patients to enterprise decision-makers) have in current voice processes and providers.  
- **Standards can only emerge from an open, neutral, and communal perspective.** Proposed standards will gain the greatest support (and eventual adoption) when such standards are developed through the leadership of an open, neutral, third party -- one dedicated to the best for the most.  The name "Open Voice Network" speaks to two core operating principles.  The first is that OVON is "open" -- transparent in its work, and neutral in its interests.  The second is that OVON is a network of voice user communities -- inclusive of aspirations and concerns from enterprises to voice developers and designers, data analysts and ethicists, marketers and media, and voice platforms worldwide.
- **Standards create trust and unlock value.** The introduction of communally-developed and broadly adopted standards and guidelines is essential to the creation of trust -- and in turn, the realization of the technology's full societal value.  

### The Open Voice Network is _Voice First_

The Open Voice Network is **Voice First.**  We are language-centric, conversationally-focused, and cognizant of the variety and richness of voice data.  We are also user-centric, inclusive (believing that voice is for all), and desirous of voice being a tool for societal good.

We recognize that voice will be increasingly combined with other digital modalities of communication and conversation, such as screen-based visuals, gestures, and facial recognition.  However: to focus our work and drive results, _we will solve voice first_. 


### From Then to Today: the Rationale for the Open Voice Network

The idea that machines could talk with and listen to humans is far from a 21st century idea.

The development of voice-based virtual assistants, often known as voicebots, reaches back at least to the 1960’s. Research into machine-based speech recognition began to bear fruit in the 1950’s. Exploration of speech synthesis – machines that could talk -- has been traced all the way back to the late 18th century. 

Given the lengthy and rich history of machines that can talk _and_ listen – a phenomenon we may today describe as voice assistance – it is vitally important that any entity wishing today to work in this space must study and understand prior research and existing standards.

Prior to second decade of this century, voicebots most often served  in interactive voice response (IVR) applications for telephony-based services, primarily in enterprise call centers.  Numerous technical standards for voice recognition and speech synthesis were developed and broadly adopted during this time.

A next era of voice – one defined by consumer-owned and -centered natural-language processing and a cloud-based solution architecture -- was heralded by the 2011 launch of Apple’s Siri voice assistant and the 2014 launch of Amazon’s Alexa assistant on the new Echo device. 

The launch of the Echo also created an entirely new consumer digital device category – the in-home smart speaker.  This device type has become the fastest-selling consumer electronics category of all time. Over the years, global annual wholesale shipments of smart speaker devices grew from roughly 9 million units in 2016 to a projected 136.9 million units in 2020 – a 72.36% CAGR.

Such success prompted others to join the consumer voice market, and to expand the use of voice assistants into a wide range of enterprise use cases.  Voice assistance – now defined largely by its new, user-owned capabilities -- now extends well beyond the call center.   It is increasingly an interface for machine and vehicle operation, factory management, and enterprise software. 

At the same time, voice assistance has – following the Apple model – become a standard feature on smartphones.  More than 500 million units of Android-enabled smartphones enjoyed voice assistant functionality as of January 2020.

Despite this remarkable growth over the past decade in capabilities, availability and adoption, it is largely believed by leading industry analysts that voice assistance is still in its early days. 

**Voice experts suggest that we will soon see:**

- **Significant advances in voice assistant capabilities.**  The user experience of voice assistance will be transformed in multiple ways.  Of most importance will be the shift from today’s command-and-question interaction to contextually-aware human-to-machine conversation.  In practice, this will enable conversation akin to that with a friend, where shared history, time, place, sentiment, and personal preferences are brought seamlessly into the flow of a discussion.   A voice assistant will not only understand what is said, but the context in which it is said and the emotion with which it is said -- and be able to respond, recommend, and remember.  Voice will also be combined with visual elements, to provide what experts term “multi-modal” conversation.  This could range from information shown on screens to the recognition of gestures and facial behaviors. 

- **Voice interfaces and conversational agents everywhere.** Voice will become ubiquitous -- the ever-device, everywhere interface to the digital world. Voice assistance will be integrated not only into devices of all types, but into places (homes, stores, banks, medical facilities, museums, stadiums) and systems for monitoring environments and processes, from factories to farms.   Analysts predict active voice assistant use on more than two billion smartphones by 2023¹, and some 8.4 billion voice assistant users worldwide by 2024².   Users will search for and speak to websites (more than 1.7 billion worldwide today³), and smart, connected objects (some 41 billion worldwide by 2025⁴.)  Futurists see a world in which voice is the normative interface for all artificial intelligence; indeed, it is predicted that all artificial intelligence will be conversational.⁵

- **Market transitions in the AI-enabled voice ecosystem and value chain.**  A unique industry is now being created by world-leading technology firms through voice assistance.  Voice is increasingly the entry point of a data-consuming engine of technology and economic growth.  Hundreds of thousands (and in time, millions) of independently-developed and purpose-built  enterprise and organizational voice assistants will emerge , each speaking and listening to constituents through a multitude of voice channels.   This will reflect an important market transition, as voice shifts from a world bounded by platform-based skills and actions to one akin to today's internet, with independent sites connecting to users across multiple global platforms.

- **The emergence of platform-independent voice assistants.**  This calls into question platform and assistant interoperability, in which an enterprise-owned independent assistant can speak with all others, regardless of platform or channel.  It suggests a standardization of components, the building blocks of voice assistants – components that have emerged from many of the standards listed below. 

- **Growing legislative and regulatory interest in voice.**  Numerous questions, on topics such as privacy, data ownership, data use, and so-called platform economics, are now being raised by western law-makers and regulators.     

In the summer of 2016, a team of academic researchers, consultants, and technology executives came together to explore the implications of this new world of voice.   The 2016-2017 research posited that this new world of voice assistance could – and, in time, would – re-shape the relationship between an enterprise and its constituents (be they shoppers, clients, or patients.) It also suggested that this new world of user-centric voice assistance represented a significant market realignment, with voice evolving from a limited-purpose technology to the interface of a new, global system of AI-enabled virtual communication.

This team was encouraged in this vision -- then, as it is now -- by the continued advancements in the technologies and linguistic research that enable voice assistance. However, it was quickly recognized that technology and linguistic innovation alone would _not_ drive voice assistance to broad enterprise acceptance and substantial market growth.  At least three other high-level dependencies were identified:

- **Enterprise trust in a level playing field:**  a widely-accepted belief that voice is a technology through which all players could benefit, and that a communally-developed "rule of law" governs the interplay between members of the voice ecosystem.   

- **Confidence in enterprise value creation, capture, and delivery:**  a widely-accepted belief -- one based upon demonstrations and documentation of use cases -- that voice assistance can and will provide a significant return on an enterprise investment.  

- **An open technology ecosystem and architecture:** the enabler of choice -- the ability to select best-of-breed technologies with the confidence of interoperability.  

These dependencies begged (then and now) for broad governing standards.  Especially standards (and protocols) that would enable the openness, user choice, inclusivity, and user trust so essential to the system’s growth and value.  

To a large degree, the state of voice assistance resembled the early, pre-standards “browser war” days of the internet.

### Vision of the Open Voice Network

**To develop and drive adoption of the technical standards and usage guidelines that will make _voice assistance worthy of user trust_.** 

Voice assistance is in its earliest days.  It is rapidly moving toward a global future that will be multi-assistant, multi-platform, multi-device,  multi-modal, and found in every industry.  It promises significant value not only to voice providers, but to its users -- the enterprises, marketers and communicators, operational managers, developers and designers, and billions of individuals worldwide.

To realize its potential, conversational AI must be open and worthy of user trust.  It must be open for development and innovation.  It must be trust-worthy in its ease and reliability use, in its direct and unfiltered access to desired destinations, in its commercial and data usage and privacy, in its cross-platform interoperability, in lack of bias, in its development languages and protocols.  

Through the centuries, the development and adoption of standards has repeatedly sparked ever-advancing use, innovation, ecosystem development, and economic growth for new technologies.  We, the sponsors and supporters of The Open Voice Network, believe that now -- at the dawn of conversational AI's multi-platform future -- is the time to communally develop and drive toward adoption the conversational AI standards that will enable user trust.  

The history of the World Wide Web (WWW) is instructive for the Open Voice Network and the future of voice assistance.  In the early days of the World Wide Web, user experience was shackled by closed implementations (e.g., AOL, etc).  Thankfully, open standards such as the Domain Name System (DNS) and industry-wide bodies such as the World Wide Web Consortium (W3C) unleashed the potential of the web, and it became the extraordinarily valuable and innovative ecosystem we know today. 

We believe that voice-enabled devices and services will exponentially expand in use and value when provided a set of open standards.  Development and adoption of such standards will prevent users from re-creating basic functionalities, and enable providers, innovators, and enterprises to focus their investments on innovation and usage differentiation.  

In addition, a voice ecosystem that is open, inclusive, and trust-worthy may also need new governance systems and capabilities, such as:

- a Voice Registry Service (VRS)
- Dialog Brokers, Dialog Managers, and other new components
- Privacy controls and standard ways to communicate and disclose this to users. 

Furthermore, like the open standards that made the World Wide Web successful, these standards would aim to have these traits:

- Open development from and through a community of regional, language, and gender diversity;
- Open management by an open committee with a neutral governing entity
- Can have an open-source reference implementation
- Can be implemented as open-source and/or closed source products and services
- Abstracted at the right level as to not too deeply specify particular technologies used to implement them

### The Guiding Values of the Open Voice Network 

Standing before the growth of conversational AI in usage and economic value are four broad issues:

**Trust, choice, inclusivity, and openness.**

**Trust** opens the door to personal and enterprise adoption, and to ever-more complex usage by all users – from individuals to developers and designers to enterprise decision-makers.  The foundation of user trust is a “rule of law” which best emerges from the neutral, industry-driven development of standards and standards-based systems.   Such a rule of law includes standards to protect the user's privacy and security, provide visibility to data and data use, and enable consent processes for data use and sharing; it also asserts standards and guidance for data use (both personal and commercial), inclusivity and accessibility, and system governance.  The Open Voice Network’s stated mission is to make voice worthy of user trust. 

**Choice** is a catalyst for ecosystem innovation and the growth of value for individuals, developers and designers, and enterprises of all sizes.  Choice is realized in many ways – in a user’s ownership and control of data, in a user’s choice of components, assistants and providers, and in the choice of assistant architecture (be it hosted locally or in the cloud.) 

**Inclusivity** is about everyone having a voice, and in every situation.  It is voice that welcomes under-represented vocalizations and dialects; voice that advances access for everyone. 

**Openness** enables interoperability among assistants and providers.  At its core is the identification and definition of standardized development building blocks.  Such blocks serve as a foundation for commercial differentiation and value; they can be assembled according to purpose, and in completely open, completely proprietary, or mixed (or hybrid) implementations. 

### OVON's Ways of Working are Based Upon Our Core Values

- We are neutral and inclusive, welcoming to participation and sponsorship all members of the conversational AI industry – from independent developers and designers to enterprise users to technology firms and platform providers.
- We seek broad participation and communal development.  No one company or person will set the direction of Open Voice Network proposed standards or usage guidelines.
- We work collaboratively with standards bodies and existing industry organizations.
- We adhere to an **open** process: collaborative, communal, balanced development; broad consensus in decision-making; transparency in decisions and documentation. 
- We seek technological solutions that 
  - can have an open-source reference implementation
  - can be implemented through open source and/or proprietary-source products and services
  - are abstracted at the right level, as to not too deeply specify particular technologies or providers
  - provide commercial providers and entrepreneurs the room necessary for innovation and differention.

### Standards and Usage Guidelines: An Aspirational List

The Technical Advisory Council of The Open Voice Network is responsible for determining OVON’s research and development of technical standards and usage guidelines. It reports to the Open Voice Network Steering Committee.

These topics are in exploration:


## Standards to enhance user interactions and capabilities

These could include:

- Log-in procedures that are privacy-friendly, fast, and easy to use.
- Platform-independent discovery and invocation of voice applications and services.
- Independent and dependable destination accessibility.
- Seamless invocation of voice applications across platforms.
- User interfaces that are consistent across human languages, dialects, and platforms.
- User interfaces that effectively use the capabilities (microphones, speakers, displays, cameras, etc.) of end point devices.
- Platform and device interoperability.
- Implementation sharing, reuse, and open improvement. 
- Back-end service choice. 

## Standards that provide privacy, safety, and security

These could include:

- User identification and authentication that is multifactor and platform independent.
- Intentional and consent-based individual and commercial privacy.
- Object control language used to specify who may perform what function with which destination, and when.

### Usage guidelines that protect privacy and enhance inclusivity

- Access that is unbiased, inclusive, and does not institutionalize racism, sexism, or ageism.
- Transparent, intentional, and consent-based data usage.
- Accurate natural language recognition across languages, dialects, gender, and age. 

There are numerous other issues which, unless addressed through standards, will continue to inhibit the trust in and use of AI-enabled voice assistance, especially as the industry evolves toward its multi-platform future.   These include:

- system architecture as it relates to the analysis and storage of data
- platform-agnostic user identification and authentication
- the lack of common voice commands for common, non-differentiating business processes (such as transaction and payment).


### Example Use Case(s) for Resolution

The Open Voice Network will test its proposed solutions against aspirational enterprise use cases from the vertical industries of the Open Voice Network:  commerce (retail and consumer goods), media, transportation, financial services, health and life sciences, public safety, connected cities, and education. 

Commerce:

The enterprise:  a multi-line retailer of goods and services with more than 50 million monthly active customers.  The customer base is 65% female, 35% male; speaks seven primary languages, with multiple dialects; and actively uses conversational AI to connect to the retailer through smartphones (40%), automobiles (30%), smart speakers (25%) and smart appliances (5%).  

Customers of this retailer use voice to establish and manage retailer club membership; place and pay for weekly replenishment orders; place and pay for unique orders; place and pay for prescription drug/pharmacy orders; place and pay for media and streaming services; manage orders and service scheduling-reservations with affiliated service providers; manage access real-time information as to order status and shipment; obtain household value information, ranging from recipes to beverage recommendations to new-born and infant care; obtain use and how-to information from branded product vendors. 

Customers will transact with this retailer using a wide range of payment options.  

The retailer considers all data within its customer interactions and transactions to be proprietary.   

### 1.0 Project Definition and Intellectual Property Licensing

Since its incorporation in May 2020, the Open Voice Network Technical Committee has operated within a Linux Foundation-chartered Project entitled **PROJECT 1.0: VOCABULARY AND DEFINITION.**

The stated remit of Project 1.0 is to name, operationally define, and prioritize the potential areas for standards development within the realm of artificial intelligence-enabled voice assistance in a manner that is mutually exclusive and collectively exhaustive (MECE), and for OVON Steering Committee review. 

The purpose of Project 1.0 is to establish a foundational floor of guidance for Technical Committee organization, talent recruitment, and resourcing.

Project 1.0 operates according to a Creative Commons (CC) Attribution 4.0 license.


### Initial Areas of OVON Standards and Usage Research

### Voice Registry System

One of the most important standards that gave rise to the World Wide Web (WWW) is the Domain Name System (DNS), the hierarchial and decentralized naming system for computers, services, or other resources connected to the Internet or a private network.  Since its initial implementation in 1985, the DNS has translated readily memorized domain names to the numerical internet protocol addresses needed for locating and identifying computer services and devices.

The need for a DNS-like system for voice, one that could connect explicit requests for specific destinations to a third-party conversational agent or voice application, has been part of OVON participant conversations since 2017. 

Soon after the incorporation of the Open Voice Network in May 2020, a Work Group was formed by the OVON Technical Committee, and tasked with evaluation of "the market/need opportunity for a global, platform-agnostic voice destination registry."  If such an opportunity was found, the Work Group was asked to "identify and define the capabilities of such a registry in preparation for the formation of an OVON Project."

**Introduction to the VRS.**

Voice Registry System (VRS) is the new concept that is part of the open-voice standards. VRS is a new global entity and, therefore, part of the pre-processing identification in the NLP. Once the NLP identifies @vrs, it passes the information to the VRS server.

The job of the VRS is to resolve the query by trying to resolve first in the root server. If not found, it is smart enough to point to the next name server, and this process continues until the request is resolved, or determined that the requested @vrs is not registered.

For more information and details about VRS, please go to [this document](https://github.com/open-voice-network/docs/blob/master/components/voice_registry_system.md).

### Voice-Specific Privacy and Security

Concerns regarding the privacy of voice assistant conversations (and the subsequent use of conversational data) has been repeatedly identified by user research and voice industry analysts as a major issue inhibiting voice use at individual and enterprise levels.  (See _Kinsella; add details here_)  In early, summer 2020 meetings of the OVON Technical Committee, it was decided to form a Work Group to _"identify, defined, and prioritze the capabilities that the OVON must 'design in' to its other work in regards to individual privacy and data security."_

In addition to its advisory role to the VRS and Architecture Work Groups, it was also determined in subsequent meetings that a Privacy and Security Work Group could bring benefit to the OVON and the industry at large by developing neutral, platform-agnostic guidelines for the protection of legislated and regulated privacy rights, and the promotion of widely-held privacy values.  

To meet this broad remit, the OVON Privacy and Security Work Group began its work in Q3 2020 with the study of the critical legislation and regulation that now governs individual privacy, as well as highly-cited third party privacy guidelines and those of leading voice assistant providers.  (A list of references follows.)  

_additional content as of 2021.01.03 to be added here_.

### Component Definition and Interface Interoperability

The interoperability of voice assistance platforms and conversational agents -- one with the ease of use of today's multi-browser internet -- is at the heart of the OVON's vision of an open, accessible voice ecosystem that is worthy of user trust. 

With this in mind, the OVON Technical Committee formed in the Summer of 2020 an Architecture Work Group, tasked to _"identify, define, and prioritize common voice assistant components for potential standardization"._

The Architecture Work Group began its efforts by studying the architectures and components of several independent voice assistant concepts, including those of the the W3C Voice Interaction Community and the Stanford Open Voice Assistant Lab (OVAL), seeking to identify and define the common "building block" components of voice assistance.   


# Components and Artifacts

Each voice agent contains several components such as those illustrated in Figure C1.  Each component consumes artifacts from previous components and produces artifacts for use by later components.  For example, the TTS component will accept text artifacts as input and produce speech artifacts as output.  Each component should have a standard format for its input and output artifacts so a component can be easily replaced or reused. 

### Flexibility

We will define the formats for artifacts (but not the algorithms that convert, consume, and produce artifacts). By standardizing artifact formats, we enable the replacement of components by other components that process input artifacts and product output artifacts faster, more efficiently, or more accurately.  For example, a speech recognition component algorithm based on the hidden Markov model can be replaced by a neural network model.

Every voice agent contains components that consume and manipulate artifacts.  If components in different voice agents produce and consume artifacts in standard formats, artifacts can be shared across voice agents.

We will define the formats for input and output artifacts of voice processing components such as ASR (Automatic Speech Recognition which converts speech to text), NLP (used to extract semantics from text produced by ASR), DM (dialog manager which processes the semantics), and TTS (which converts text to spech).  Note that we do not specify the algorithms inside each component; technology vendors compete by providing alternative algorithms for components.    
Voice agent developers can select components from multiple vendors that fill the developers specific needs, and connect the selected components together because they communicate with each other using standardized artifacts.

Here are some examples:

- In a smart home environment, it may be desirable to add speech recognition and encoding software to a microphone. (Figure C2)  The microphone converts speech to digital text which is then encoded for security purposes before transmission to a network hub for further processing.  Because text is more compact than speech, this approach saves transmission bandwidth.  
TTSs are often designed to perform well with a specific national language (North American English, German, etc.) 
- Figure C3 illustrates the replacement of an North American English TTS by a German TTS which pronounces German better than the English TTS
- In order to accommodate a handicapped person who can not speak or hear well, the microphone and speaker are replaced by a keyboard and display in Figure C4.

### Interoperability

Interoperability enables one voice agent to use the data and functions of another voice agent.  For example, a shopping agent collects user account information and then invokes a second agent, the validity agent.  Information collected during the shopping agent needs to be copied to the validity agent in order to avoid having the validity agent solicit same information from the user again.

In a more detailed example, Figure C5 illustrates the intents for an airline reservation voice agent and a hotel reservation voice agents and the resulting user dialogs.  Note that the redundant turns (denoted in colors) in the user dialogs.  The redundant turns can be removed from the user dialogs by copying slot values from the airline reservation voice agent to slots within the hotel reservation voice agent.  

### Component Framework

In order to manage the big task of identifying and recommending standard formats for artifacts, we partition components into four baskets inspired by the advent and progress of automotive transportation.  High-level descriptions of these four baskets and their possible components follows. (Figure C6)

#### 1. Macadam Road “Foundational” Component Basket (Core functionality)

Named after the 18th century Scottish engineer who invented the hard-surface, reliable road.  This basket of components provides the basic infrastructure needed by voice agents.

ASR, Automatic Speech Recognition

- Purpose: Convert speech to text 
- Input: audio
- Output: text
- May interact with: Context Manager
- May interact with: Knowledge Manager

NLU, Natural Language Understanding (Intent Extraction)

- Purpose: extract meaning from text
- Input: text
- Output: semantic representation of text
- Note 1: known as semantic interpretation in VoiceXML
- Note 2: ASR + NLU often combined into a combo component 
- May interact with: Context Manager
- May interact with: Knowledge Manager

Dialog Manager

- Purpose: respond to request
- Input: semantic interpretation of text
- May interact with: Context Manager
- May interact with: Knowledge Manager
- Interacts with: Locator Service (VRS) to obtain link to voice agent
- Interacts with: Fulfillment (Dialog) Broker to obtain links to backend apps
- Interacts with: backend apps to obtain fulfillment
- Interacts with: NLG Generator to formulate text response

TTS, Text To Speech

- Purpose: Convert text to speech 
- Input: text (possibly with hints for pronunciation)
- Output: audio

#### 2. Ford Model T “Foundational” Component Basket (Core functionality + OVON unique value)

Named after the Model T, a  practical, affordable transportation for the common man which quickly became prized for its low cost, durability, versatility, and ease of maintenance.  This basket of components provides core functionality and OVON unique value. 

VRS Locator Service

- Purpose 1:  Maintain links to speech agents
  - Input: pronunciation of speech agent name
  - Output: link to speech agent
- Purpose 2:  Register speech agent name
  - Intent (Dialog) Broker
  - Input: pronunciation of speech agent name & information
  - Output: Success or failure

Fulfillment Broker

- Purpose:  Provide a list of fulfillable intents
Input: request
Output: fulfillment info  
Note: previously called Dialog Broker
- Context Manager
Purpose: Maintains history and context of the conversation
Input: TBD
Output: TBD



### 3. ’57 Chevy  “Desirable” Component Basket: enhanced functionality, safety, UX design

Named after the '57 Chevy, a popular and fun car that was easy and cheap to work on. This phase provides enhanced functionality and safety.  Example components include Human speaker ID and Natural Language Result Generation of text for results and warning messages.

- Human speaker identification 
Purpose: Identify human speaker by using voice prints
Input: audio
Output: speaker identification
Note: This component is frequently combined with ASR to form a combo-component
Note: other components including ARS, NLU, Dialog manager, NLG may access the contents of this component.  The persistence of this component it TBD
- Knowledge Manager 
Purpose: maintains real world knowledge and common knowledge in the form of ontologies and other data structures TBD
Input: TBD
Output: TBD
Note: other components including ARS, NLU, Dialog manager, NLG may access the contents of this component.  
Note: This is a new component suggested by Jonathan but not discussed on 10/21
- NLG result generator
Purpose: Generate text and error/warning 
Input: semantic information
Output: text
Note: may interact with personalization information (part of the user session manager) to personalize messages to the user
Note: may be extended to support language translation
- Access Control Guard
Purpose: controls access to data and functions
Input: access control constraints (Boolean expression involving the functions and parameters of an API to an object (such as a backend app, a voice agent, etc.), and system parameters (date, time, etc.)
Output: permission granted (or not) to access data and/or functions
Note: inspired by the Almond access control mechanism

- User Session Manager
Purpose Establish and maintain environment parameters (national language preference, recording options, wake-up words, personal NLP and other profiles information TBD.
Input TBD
Output TBD
Note:  ON 10/21 Dan suggested “profile” which fits in this component

### 4. Tesla “Visionary Component Basket” redefinition and technology advancement”

Named after Tesla that demonstrates the electric vehicles can be better, quicker and more fun to drive than gasoline cars.  This basket of components hprovides new technologies and capabilities.  
- Emotion detection 
Purpose: extract a human user’s emotion from their voice 
Input: voice
Output: one or more emotions with a rating for each emotion indicating its strength
Note: There is a second technology for emotion: analysis of wording and phrasing of text.  This may be performed in the NLU component.

- Plan Generator
Purpose: Convert a high level request into a plan (a sequence of invocations of agents needed to complete the request
Input: high-level user request
Output:  Plan involving multiple agents
Note: there are several strategies for developing plans.  Venders will differentiate themselves by implementing difference strategies for different situations.  
- Result Generator on steroids
Purpose: extend the NGL result generator to include multi-modal, multi-media conversations

## Design

### Vocabulary

See [OVON vocabularies](https://github.com/open-voice-network/docs/blob/master/vocabulary.md).

### Examples

Several examples that are useful to explain all the concepts required in the design:

"Computer, please add milk to my shopping list at BigGrocery"

- machine translates to → {wakeword}, {query} {vrs}
- intent: addShoppingList
- entities: milk = @product

"Computer, ask BigGrocery to add milk to my shopping list"

- translates to → {wakeword}, {vrs} {query}
- intent: addShoppingList
- entities: milk = @product

### Component Architecture

![Component Architecture](assets/component_architecture_diagram.png "Fig. 1 - Component Architecture Diagram")

> Note: Dialog Broker, VRS, and Dialog Manager are new concepts. NLP, TTS / STT, and Channels are things that already exist, but we list them because they will be affected and influenced by the standards.

### Component Flow

This component flow section aims to describe the general steps that happen in and between each component.

![Sequence Diagram](assets/component_flow_sequence_diagram.png "Fig. 2 - Component Flow Sequence Diagram")

1. "{wake word}, add milk to my shopping list at BigGrocery."
2. NLP calls VRS and passes the query. NLP sends in the query its location and what it thinks the VRS name lookup should resolve for (in the case below it understands that "biggrocery" is what the VRS lookup is for). See Figure 1.0.

```json
{
  "query": "add milk to my shopping list at biggrocery",
  "vrs_name_lookups": [
    { "vrs_name": "biggrocery"}
  ],
  "channel_location": {
    "lat": "",
    "long": ""
  },
  "channel": "app",
  "source_nlp_provider": "AmazonLex"
}
```

3. The name lookup of the "biggrocery" is resolved in VRS. It will respond with the records for the dialog broker, dialog manager, and NLP for the given location(s) that were requested in the lookup. See Figure 2.0 and Figure 2.1.
4. NLP will call the Dialog Broker based on the endpoint it receives from the VRS. See Figure 3.0
5. NLP identifies the intent and entities. If identified intent is not part of the fulfillable intents, NLP will do a default fail response.

```json
{
  "intent": "addShoppingList",
  "entities": [
    { "@product" }
  ],
  "query": "add milk to my list at biggrocery"
}
```

6. If VRS returns the Dialog Manager endpoint, NLP will call the Dialog Manager to have a better dynamic response for the vendor.

### Component Details

This component details section aims to further define the individual components in more detail.

### Utterance

An utterance construct is: {wake word}, {query}

### Wake Word

Wake word options:

- Static wake word
- Trainable static wake word
- Custom trainable wake word (e.g. instead of "Hey Google", "Hey Johnny")
- Add another ecosystem’s wake word (Hey Google, Hey Alexa, Hey Siri, etc) to a device in addition to the wake word(s) already on the device (this would result in more than 1 wake word being on the device). This would require a standard given it’s an interface that would need standardization.

### Query

The words that the user is using after the wake word. The query can have multiple constructs.

Examples:

- {invocation}
- {launch phrase} {explicit invocation}
- {launch phrase} {implicit invocation}

### Channel

Channel is the physical or virtual interface to the user where the dialog between the computer and the user occurs (e.g. a physical device, a web page, apps, etc). It is responsible for receiving voice input from a user and delivering back a potentially personalized response using inputs from TTS and the Dialog Manager. It manages the specific native framework such as cards to provide a better user experience and address some visual nuisances to individual channels.

### STT / TTS

Speech To Text (STT) is the component of the architecture that converts the audio to text. It applies a deep-learning AI algorithm to apply knowledge about noise robustness, phrase hints, spelling, language structure, and global vocabulary of any utterances.

Text To Speech (TTS) is the opposite of the STT. It translates a text to a more synthesized natural-sounding speech. The TTS is the component that creates a simulated conversation with the user.

Both STT, and TTS can be implemented on the cloud, on-premise, or embedded devices based on what the technology offers.

There are multiple known STT / TTS technologies in the industry such as Watson STT, Google Cloud, Microsoft, and Mozilla Deepspeech.

This is another domain where OVON can play a significant role in influencing the standard schema for all the STT / TTS technologies players.

_Previously-developed and broadly-adopted standards that have shaped STT and TTS development include Web Speech API (a JavaScript API that allows web developers to incorporate speech recognition and synthesis into their web pages), Speech Synthesis Markup Language (SSML) Version 1.0 (a standard way to control presentation of speech such as pronunciation, volume, pitch rate, etc. across different synthesis-capable platforms), Proununciation Lexicon Specification Version 1.0 (a syntax for specifiying pronunciation lexicons to be used by Automatic Speech Recognition and Speech Synthesis engines in voice browser applications), Emotion Markup Language (Emotion ML) 1.0 (a "plug-in" language suiteable for use in three different areas:  1) manual annotation of data, 2) automatic recognition of emotion-related stsates from user behavior; and 3) generation of emotion-related system behavior), anbd ToBI (a set of conventions for transcribing and annotating the prosody of speech.)._  

### NLP

As we enter the era of human-machine conversation, the crown jewel component of it is Natural Language Processing. The problem we are facing today is the lack of standards from different NLP providers. In the world of open-standard, we are introducing the concept of a global entity, i.e., `@vrsname`. It is not about primitive data types that individual NLP understands but a smart object that all NLP providers understand. In the pre-processing stage of NLP, it is evaluating the query as a whole and identifying if any global identity exists in the query. The identified global entity is submitted to the VRS.

The global identity solves the interoperability aspect of NLPs. For example, the word "BigGrocery" in Amazon NLP is going to be treated the same way in Microsoft Luis or Einstein. Like domain names, this gives businesses the flexibility to be handled identically in the world of the human-machine conversation regardless of the NLP provider they choose.

NLP schema is another place where OVON can influence the standard. Different NLPs have their own set of schema that leads to the different implementation of standards. Some NLPs understand the concept of entity, but some do not. Some understand the intent, and some have different flavors of it and call it "action." Because of the closed implementations, each business has to orchestrate this separately or build their abstracted implementation version to be able to adapt to different NLP standards. OVON is proposing to set a standard schema for all NLP to follow:

```json
{
  "response_id": UUID,
  "query": "add one milk to my shopping list at biggrocery", // raw version of the user inquiry
  "query_result": {
    "intents": [ // list of intents related to the query
      {
        "name": "addShoppingList",
        "display_name": "add shopping list",
        "confidence_score": 0.9999,
        "is_top_score": "true"
      },
      {
        "name": "orderList",
        "display_name":"order shopping"
        "confidence_score": 0.7982
        "entities": [ //list of entities related to the query
      {
        "one": "sys.integer"
      },
      {
        "milk": "@product"
      }
    ],
    "is_required_parameters_present": "true", //boolean value if all required parameters in the top intent are supplied
    "history_context": {
    …
    }, //array of intents and entities from previous intents.
    "response": [ //array of intents and entities from previous intents
      {
        "text": "one milk is added to your shopping list at BigGrocery",
        "default_response": ""
      }
    ],
  }
  "vrs_name_lookups": [
    {
      "vrs_name": "biggrocery"
    }
  ],
  "channel_location": {
    "lat": "",
    "long": ""
  },
  "channel": "app",
  "source_nlp_provider": "microsoft",
  "language": "en"
}
```

> Previously-developed and broadly-adopted standards that have shaped Automatic
> Speech Recognition implementations to date include Web Speech API (a
> JavaScript API to enable web developers to incorporate speech recognition and
> synthesis into their web pages. It enables developers to use scripting to
> generate text-to-speech output and to use speech recognition as an input for
> forms, continuous dictation and control); Speech Recognition Grammar
> Specification Version 1.0 (a syntax for representing grammars for use in
> speech recognition so that developers can specify the words and patterns of
> words to be listened for by a speech recognizer); Pronunciation Lexicon
> Specification Version 1.0 (a syntax for specifying pronunciation lexicons to
> be used by Automatic Speech Recognition and Speech Synthesis engines in voice
> browser applications); Semantic Interpretation for Speech Recognition (SISR)
> Version 1.0 (the process of Semantic Interpretation for Speech Recognition and
> the syntax and semantics of semantic interpretation tags that can be added to
> speech recognition grammars to compute information to return to an application
> on the basis of rules and tokens that were matched by the speech recognizer.
> In particular, it defines the syntax and semantics of the contents of Tags in
> the Speech Recognition Grammar Specification) and International Phonetic
> Alphabet (often used to represent the sound of words in lexicons.)_


> Previously-developed and broadly-adopted standards that have shaped Core
> Natural Language Understanding implementations to date include EMMA:
> Extensible MultiModal Annotation markup language Version 2.0 (a set of
> specifications for multimodal systems providing details of an XML markup
> language for containing and annotating the interpretation of user input and
> production of system output), JSON Representation of Semantic Information (a
> JSON format for representing the results of semantic processing), Abstract
> Meaning Representation (used for semantic representation language. Is a
> culmination of prior researchers and projects and could become how semantic
> meaning of utterances is represented in the future); and ARPA N-Gram format (a
> published format to portably represent language models in N-Gram format.)_


### Dialog Broker

The Dialog Broker provides a list of fulfillable intents.

```json
{
  "dialog_broker_id": UUID,
  "url": "biggrocery.com/dialog_broker",
  "fulfillable_intents": [
     # some object(s) here that defined a fulfillable intent
     # add to shopping list
     # browse products
     # etc
  ]
}
```

TODO: add more details here

### Dialog Manager

Manages the actual context of the conversation. It is responsible for choosing the best action to perform the conversation.

```json
{
  "response_id": UUID,
  "query_result": {
    ...
  }
}
```

> There are several previously-developed and broadly adopted standards which may
> be of relevance to the Dialog Manager development. OVON research in Q4 2020
> suggests the following: Voice Extensible Markup Language (VoiceXML) 2.1
> (designed for creating audio dialogs that feature synthesized speech,
> digitized audio, recognition of spoken and DTMF key input, recording of spoken
> input, telephony, and mixed initiative conversations. Its major goal is to
> bring the advantages of Web-based development and content delivery to
> interactive voice response applications), and State Chart XML (SCXML) (which
> provides a generic state-machine based execution environment based on CCXML
> and Harel State Tables.)

### Channel Registry

TODO: add introduction and more details here; add to diagrams and flows above

TODO: clean up this bulleted list; it’s just a placeholder of things to consider for now.

- Channel registry should work be implementable on any private network OR be a SaaS service provided by a provider
- Each channel can be a part of a channel registry
- A channel in a channel registry should be able to be made aware of other channels registered in the channel registry
- A channel fetches its configuration from the channel registry
- Configuration in the channel registry should specify a standard for mandatory configuration items, optional configuration items, or allow flexibility for user-defined configuration items (ex. kind of like how HTTP headers are)
- Optional configuration items:
- SST engine to be used by the channel

## Vertical Dependencies

Items in the vertical dependencies need to be addressed in each architectural component.

# Privacy

Part of the principles of OVON is privacy. Below are the guidelines to focus on:

- Always end-users interest first. This is essential, especially for end-users who are marginalized or vulnerable sectors of the society who may not be aware of how their data are being collected, used, or shared. 
- Always ask. Get an informed acknowledgment from the end-user, before collecting, sharing, or using the data.
- Provide an audit trail for personal or sensitive information captured.
- Always communicate to the end-user the data being collected and informed the purpose of the collection of data.

# Security

FIXME

# Context


## Addendums

TODO

### Visual Displays

TODO



### Common Commands for Common Processes

TODO

### Platform-Agnostic User Identification and Authentication

TODO

### Platform Interoperability


## References

- Fig. 1 - Component Architecture Diagram: <https://docs.google.com/drawings/d/1ELxRKqyaUCWRoc5ein_ajHsy3Y_ZDpY9ErIfGT0lYq4/edit?usp=sharing>
- Fig. 2 - Component Flow Sequence Diagram: <https://docs.google.com/drawings/d/1rMfX6-oN81t27WD6Dbkd6Bee1KDYkHWEXPb-tNn0ykQ/edit?usp=sharing> 
